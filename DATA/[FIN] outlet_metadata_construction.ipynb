{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnfVxO79IeScY26WGfvTT2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This file takes in data from hugging face and produces the metadata for each news outlet, exporting it to a set pathname."],"metadata":{"id":"AUmEZaOqHe0f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9acSNJQktsE"},"outputs":[],"source":["# Imports\n","import os\n","from google.colab import drive\n","from tqdm import tqdm\n","from collections import defaultdict\n","import ast\n","!pip install datasets > /dev/null 2>&1\n","from datasets import load_dataset\n","import pandas as pd\n","from datetime import datetime\n","import requests\n","import time\n","import numpy as np\n","import pickle\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/STANFORD/SENIOR (2024-2025)/CS224W/cs224w_project')"]},{"cell_type":"code","source":["# EDIT ACCORDINGLY\n","DECADE = 60\n","TOPIC = \"labor\"\n","\n","full_decade_data = load_dataset(f\"amyguan/newswire-{DECADE}-{DECADE+10}\")['train']\n","topic_decade_data = load_dataset(f\"pnsahoo/{DECADE}-{DECADE+10}-{TOPIC}-embedding\")['train']\n","outlet_metadata_filename = f\"karsen_redo/DATA/outlet_metadata_{DECADE}{DECADE+10}_{TOPIC}.pkl\"\n","\n","api_key = \"INSERT API KEY\"\n","\n","\n","def get_coordinates_google(city, state, api_key):\n","    \"\"\"\n","    Get the coordinates of a city and state using the Google Maps Geocoding API.\n","    \"\"\"\n","    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={city},{state}&key={api_key}\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        results = response.json().get('results')\n","        if results:\n","            location = results[0]['geometry']['location']\n","            return location['lat'], location['lng']\n","    return None\n","\n","def find_outlet_metadata_decade(full_decade_data):\n","  \"\"\"\n","  This finds the main outlet metadata for all topics in the decade.\n","  \"\"\"\n","  # Initialize variables\n","  outlets = set()\n","  articles_per_outlet = defaultdict(int)\n","  articles_per_ca_topic = defaultdict(lambda: defaultdict(int))\n","  wire_coordinates_per_outlet = defaultdict(set)\n","  city_per_outlet = {}\n","  state_per_outlet = {}\n","  newspaper_coordinates_per_outlet = {}\n","\n","  batch_size = 1000\n","\n","  # Iterate through all articles\n","  for i in tqdm(range(0, len(full_decade_data), batch_size), desc=\"Processing batches\"):\n","      batch = full_decade_data[i: i + batch_size]\n","      l = len(batch['article']) # Number of articles in this batch\n","\n","      for j in range(l):\n","          ca_topic = batch['ca_topic'][j]\n","          wire_coord = tuple(batch['wire_coordinates'][j])\n","          row = batch['newspaper_metadata'][j]\n","\n","          for outlet in row: # Per newspaper\n","              names = ast.literal_eval(outlet['newspaper_title'])\n","              cities = ast.literal_eval(outlet['newspaper_city'])\n","              states = ast.literal_eval(outlet['newspaper_state'])\n","\n","              if names:\n","                  outlet_name = names[0] # Only look at first newspapername in the metadata if multiple names\n","                  if city_per_outlet.get(outlet_name) is None:\n","                      city_per_outlet[outlet_name] = cities[0]\n","                      state_per_outlet[outlet_name] = states[0]\n","                      coordinates = get_coordinates_google(cities[0], states[0], api_key)\n","                      time.sleep(0.1) # avoid overwhelming the geocoding service\n","                      if coordinates:\n","                          newspaper_coordinates_per_outlet[outlet_name] = coordinates\n","                      else:\n","                          newspaper_coordinates_per_outlet[outlet_name] = (0, 0)\n","\n","                  outlets.add(outlet_name)\n","                  articles_per_outlet[outlet_name] += 1\n","                  articles_per_ca_topic[outlet_name][ca_topic] += 1\n","                  wire_coordinates_per_outlet[outlet_name].add(wire_coord)\n","  return outlets, articles_per_outlet, articles_per_ca_topic, wire_coordinates_per_outlet, city_per_outlet, state_per_outlet, newspaper_coordinates_per_outlet\n","\n","def compute_average_embeddings_topic(topic_decade_data):\n","  \"\"\"\n","  This computes the average article embedding for each outlet in the topic-decade, which is later used in outlet metadata.\n","  \"\"\"\n","  embeddings_per_outlet = defaultdict(list)\n","  batch_size = 100\n","\n","  # Iterate through articles\n","  for i in tqdm(range(0, len(topic_decade_data), batch_size), desc=\"Processing embeddings\"):\n","      batch = topic_decade_data[i: i + batch_size]\n","      l = len(batch['article']) # Number of articles in this batch\n","      for j in range(l):\n","          embeddings = batch['embedding'][j]\n","          row = batch['newspaper_metadata'][j]\n","          for outlet in row:\n","              names = ast.literal_eval(outlet['newspaper_title'])\n","              if names:\n","                  embeddings_per_outlet[names[0]].append(embeddings)\n","\n","  average_embeddings_per_outlet = {k: np.mean(v, axis=0) for k, v in embeddings_per_outlet.items()}\n","  final_outlets = list(average_embeddings_per_outlet.keys())\n","  return final_outlets, average_embeddings_per_outlet\n","\n","def compute_outlet_metadata(topic_decade_data):\n","  \"\"\"\n","  This computes the outlet metadata for the topic-decade, calling on other functions as needed.\n","  \"\"\"\n","  # Compute only for outlets in the topic-decade\n","  final_outlets, average_embeddings_per_outlet = compute_average_embeddings_topic(topic_decade_data)\n","\n","  # Construct outlet metadata in dataframe\n","  outlet_metadata = []\n","  for outlet in final_outlets:\n","      num_articles = articles_per_outlet[outlet]\n","      ca_topic_distribution = articles_per_ca_topic[outlet]\n","\n","      ca_topic_shares = {f\"share_{topic}\": count / num_articles for topic, count in ca_topic_distribution.items()}\n","\n","      num_unique_wire_sources = len(wire_coordinates_per_outlet[outlet])\n","      wire_coord_diversity = num_unique_wire_sources / num_articles if num_articles > 0 else 0\n","\n","      average_embedding = average_embeddings_per_outlet[outlet]\n","      city = city_per_outlet[outlet]\n","      state = state_per_outlet[outlet]\n","      newspaper_coordinates = newspaper_coordinates_per_outlet[outlet]\n","\n","      outlet_metadata.append({\n","          'outlet_name': outlet,\n","          'num_articles': num_articles,\n","          'wire_coord_diversity': wire_coord_diversity,\n","          'newspaper_city': city,\n","          'newspaper_state': state,\n","          'newspaper_coordinates': newspaper_coordinates,\n","          **ca_topic_shares,\n","          'avg_embedding': average_embedding\n","      })\n","\n","  # Export metadata\n","  outlet_metadata_df = pd.DataFrame(outlet_metadata)\n","\n","  # Fill NaNs with 0 (in case some outlets don't have articles in specific topics)\n","  outlet_metadata_df.fillna(0, inplace=True)\n","  outlet_metadata_df.to_pickle(outlet_metadata_filename) # CHANGE NAME if different\n"],"metadata":{"id":"Xst8Q-BnlFY1"},"execution_count":null,"outputs":[]}]}