{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This code takes in a homogeneous graph and trains a GNN based on link prediction, with either GCN or GAT and using negative sampling with binary cross-entropy loss. It exports the resulting embeddings and loss plots."],"metadata":{"id":"q8ElJ2IOFvo2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cx-2kUZjYUjs"},"outputs":[],"source":["# Imports\n","import os\n","from google.colab import drive\n","import torch\n","import torch.nn.functional as F\n","!pip install torch_geometric > /dev/null 2>&1\n","!pip install pytorch_lightning > /dev/null 2>&1\n","from torch_geometric.nn import GCNConv, GATConv\n","from torch_geometric.utils import negative_sampling\n","import pytorch_lightning as pl\n","from torch_geometric.data import DataLoader\n","import matplotlib.pyplot as plt\n","import glob\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/STANFORD/SENIOR (2024-2025)/CS224W/cs224w_project')"]},{"cell_type":"code","source":["def find_filepath(decade, topic):\n","    \"\"\"\n","    Given a decade and topic, this helper function finds the filepath of the plot.\n","    \"\"\"\n","    pattern = f'karsen_redo/HOMOGNN/homogeneous_graph-{decade}-{decade+10}-{topic}*.pt'\n","\n","    # Use glob to find files matching the pattern\n","    filepaths = glob.glob(pattern)\n","\n","    if filepaths:\n","        return filepaths[0]  # Return the first match\n","    else:\n","        return None\n"],"metadata":{"id":"bOck-WGx7DHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","This defines the class used for the GNN. It is unsupervised and based on link prediction, and uses negative sampling with binary cross-entropy loss. It has two layers, either GCN or GAT.\n","\"\"\"\n","class LinkPredictionGNN(pl.LightningModule):\n","    def __init__(self, input_dim, hidden_dim, output_dim, lr):\n","        super().__init__()\n","        # GNN layers\n","        # Change to GATConv if using GAT\n","        self.conv1 = GCNConv(input_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, output_dim)\n","\n","        # Decoder for link prediction\n","        self.decoder = torch.nn.Bilinear(output_dim, output_dim, 1)\n","        self.losses = []\n","        self.lr = lr\n","\n","    def forward(self, x, edge_index):\n","        # First GNN layer\n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training)\n","\n","        # Second GNN layer\n","        x = self.conv2(x, edge_index)\n","\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        # Access x and edge_index directly from the batch (graph)\n","        x, edge_index = batch.x, batch.edge_index\n","\n","        # Learn node embeddings\n","        z = self.forward(x, edge_index)\n","\n","        # Positive edges (same as edge_index)\n","        pos_edge_index = edge_index\n","\n","        # Negative sampling\n","        neg_edge_index = negative_sampling(\n","            edge_index,\n","            num_nodes=x.size(0),\n","            num_neg_samples=pos_edge_index.size(1)\n","        )\n","\n","        # Compute link prediction loss\n","        pos_pred = self.decoder(\n","            z[pos_edge_index[0]],\n","            z[pos_edge_index[1]]\n","        ).squeeze()\n","        neg_pred = self.decoder(\n","            z[neg_edge_index[0]],\n","            z[neg_edge_index[1]]\n","        ).squeeze()\n","\n","        # Binary cross-entropy loss\n","        pos_loss = F.binary_cross_entropy_with_logits(pos_pred, torch.ones_like(pos_pred))\n","        neg_loss = F.binary_cross_entropy_with_logits(neg_pred, torch.zeros_like(neg_pred))\n","\n","        loss = pos_loss + neg_loss\n","\n","        self.losses.append(loss.item())\n","        print(f\"Step {batch_idx}: train_loss = {loss.item()}\")  # Print the loss\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), self.lr)\n","\n","\n","def train_link_prediction_gnn(graph, hidden_dim, output_dim, lr):\n","    \"\"\"\"\n","    Given the graph and a set of parameters, this function trains the GNN and returns the final node embeddings.\n","    \"\"\"\n","    # Initialize model\n","    model = LinkPredictionGNN(input_dim=graph.x.shape[1], hidden_dim=hidden_dim, output_dim=output_dim, lr=lr)  # Use graph.x to get input dimension\n","\n","    # Convert graph to a DataLoader (since there's only one graph, we wrap it in a list)\n","    train_loader = DataLoader([graph], batch_size=1)\n","\n","    # Lightning Trainer\n","    trainer = pl.Trainer(\n","        max_epochs=100,\n","        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n","        devices=1,\n","        log_every_n_steps=5,  # Log every 5 steps\n","        callbacks=[\n","            pl.callbacks.EarlyStopping(monitor='train_loss', patience=10),\n","            pl.callbacks.ModelCheckpoint(monitor='train_loss')\n","        ]\n","    )\n","\n","    # Train the model using the DataLoader\n","    trainer.fit(model, train_dataloaders=train_loader)  # Use DataLoader here\n","\n","    # Get final node embeddings\n","    with torch.no_grad():\n","        final_embeddings = model(graph.x, graph.edge_index)  # Use graph.x and graph.edge_index directly\n","\n","    return final_embeddings, model"],"metadata":{"id":"yEy59AaLaDZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_homo_gnn(TOPIC, DECADE, hidden_dim, output_dim, lr, embeddings_path_name, newspaper_path_name):\n","  \"\"\"\n","  This function runs the homogeneous graph GNN, taking in a set of parameters and paths that determines which graph is used and what parameters are used in the GNN.\n","  \"\"\"\n","  file_path = find_filepath(DECADE, TOPIC)\n","  filename = os.path.basename(file_path)\n","  num_newspapers = int(filename.split(\"-\")[-1].split(\".\")[0])\n","  graph = torch.load(file_path)\n","  node_embeddings, trained_model = train_link_prediction_gnn(graph, hidden_dim, output_dim, lr)\n","\n","\n","  torch.save(node_embeddings, f'{embeddings_path_name}.pt')\n","  torch.save(node_embeddings[:num_newspapers], f'newspaper_node_embeddings-{DECADE}-{DECADE+10}-{TOPIC}') # Isolate newspaper embeddings from the node embeddings\n","\n","  epochs = range(1, len(trained_model.losses) + 1)  # Epochs are index + 1\n","\n","  # Plot loss function\n","  plt.plot(epochs, trained_model.losses)\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.title('Training Loss over Epochs')\n","  plt.legend()\n","  plt.savefig(f'{embeddings_path_name}.png')\n","  plt.show()"],"metadata":{"id":"pi2APqP7aN5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","This cell trains the GNN for a set of parameters and saves the resulting embeddings and loss plots in predetermined paths.\n","\"\"\"\n","\n","conv = 'gcn'\n","sizes = [(64, 32), (128, 64)]\n","num_layers = 2\n","lr = 0.005\n","\n","for decade in [60]:\n","  for topic in ['labor', 'civil-rights', 'macro']:\n","    for hidden_size, out_size in sizes:\n","        print(f'\\n DECADE {decade}, TOPIC {topic}, HIDDEN SIZE {hidden_size}, OUTPUT SIZE {out_size}')\n","        embeddings_path_name = f'karsen_redo/HOMOGNN/{topic}_{decade}{decade+10}/homo_{conv}_h{hidden_size}_o{out_size}_l{num_layers}_lr{lr*1000}'\n","        newspaper_path_name = f'karsen_redo/HOMOGNN/{topic}_{decade}{decade+10}/newspaper_embeds-{conv}_h{hidden_size}_o{out_size}_l{num_layers}_lr{lr*1000}.pt'\n","\n","\n","          # Train the GNN and get embeddings\n","        run_homo_gnn(topic, decade, hidden_size, out_size, lr, embeddings_path_name, newspaper_path_name)"],"metadata":{"id":"ajFYVfuUsbI1"},"execution_count":null,"outputs":[]}]}