{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This code generates the homogeneous graph for a newswire data subset as chosen, and exports it."],"metadata":{"id":"YqMPuThCEuyc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dN4LSl0SKIs"},"outputs":[],"source":["# Imports\n","\n","import os\n","import ast\n","from google.colab import drive\n","from tqdm import tqdm\n","from collections import defaultdict\n","import ast\n","!pip install datasets > /dev/null 2>&1\n","from datasets import load_dataset\n","import pandas as pd\n","from datetime import datetime\n","import numpy as np\n","import pickle\n","import torch\n","import torch_geometric as pyg\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/STANFORD/SENIOR (2024-2025)/CS224W/cs224w_project')"]},{"cell_type":"code","source":["def prepare_features(newspaper_df, topic_data):\n","    \"\"\"\n","    Given a newswire dataset and newspaper metadata, this function creates the newspaper and article features and concatenates them into a single dataframe.\n","    \"\"\"\n","    # Newspaper features\n","    numeric_columns = [col for col in newspaper_df.columns if col not in ['outlet_name', 'avg_embedding', 'city', 'state', 'newspaper_city', 'newspaper_state', 'newspaper_coordinates']]\n","    newspaper_features_df = newspaper_df[numeric_columns]\n","\n","    # Article features\n","    embeddings = []\n","    wire_coordinates = []\n","    date_features = []\n","\n","    for article in topic_data:\n","        embeddings.append(article['embedding'])\n","        wire_coordinates.append(article['wire_coordinates'])\n","\n","        # Find the earliest date\n","        date_list = article['dates']\n","        if date_list:\n","            earliest_date = min(datetime.strptime(date, '%b-%d-%Y') for date in date_list)\n","            earliest_timestamp = earliest_date.timestamp()\n","        else:\n","            earliest_timestamp = 0.0\n","\n","        date_features.append(earliest_timestamp)\n","\n","    # Add node type column to differentiate nodes\n","    newspaper_features_df['node_type'] = 0  # 0 for newspaper\n","\n","    embeddings_df = pd.DataFrame(embeddings)\n","    wire_coordinates_df = pd.DataFrame(wire_coordinates, columns=['latitude', 'longitude'])\n","    date_features_df = pd.DataFrame(date_features, columns=['earliest_timestamp'])\n","\n","    article_features_df = pd.concat([\n","        embeddings_df,\n","        wire_coordinates_df,\n","        date_features_df\n","    ], axis=1)\n","    article_features_df['node_type'] = 1  # 1 for article\n","\n","    # Combine features\n","    combined_features_df = pd.concat([newspaper_features_df, article_features_df], ignore_index=True)\n","    combined_features_df = combined_features_df.fillna(0)\n","\n","    return combined_features_df\n","\n","def process_features(combined_features_df, n_components=50):\n","    \"\"\"\n","    This function scales and applies PCA to the combined features.\n","    \"\"\"\n","    # Separate features for scaling and PCA\n","    combined_features_df.columns = combined_features_df.columns.astype(str)\n","    features_to_scale = combined_features_df.select_dtypes(include=['float64', 'float32'])\n","    node_type = combined_features_df['node_type']\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    scaled_features = scaler.fit_transform(features_to_scale)\n","\n","    # Apply PCA\n","    pca = PCA(n_components=n_components)\n","    pca_features = pca.fit_transform(scaled_features)\n","\n","    # Combine PCA features with node type\n","    processed_features = torch.tensor(pca_features, dtype=torch.float)\n","    node_type_tensor = torch.tensor(node_type.values, dtype=torch.long)\n","\n","    return processed_features, node_type_tensor\n","\n","def create_homogeneous_graph(topic_data, newspaper_df):\n","    \"\"\"\n","    This function takes in the newswire data and outlet metadata and creates a homogeneous graph from the combined features.\n","    \"\"\"\n","    # Prepare combined features\n","    combined_features_df = prepare_features(newspaper_df, topic_data)\n","\n","    # Process features with PCA\n","    node_features, node_type = process_features(combined_features_df)\n","\n","    # Create edge index\n","    num_rows = len(newspaper_df)\n","    # DICTIONARY with one node index dict, newspapers first, add length of newspapers to the article indices\n","\n","    newspaper_indices = {}\n","    for i, row in newspaper_df.iterrows():\n","      newspaper_indices[row['outlet_name']] = i\n","\n","    num_newspapers = newspaper_df.shape[0]\n","\n","    edges = []\n","\n","    for idx, article in enumerate(topic_data):\n","        article_idx = idx + num_newspapers\n","        for newspaper in article[\"newspaper_metadata\"]:\n","            newspaper_tuple = ast.literal_eval(newspaper['newspaper_title'])\n","            newspaper_title = newspaper_tuple[0].strip().lower()\n","\n","            if newspaper_title in newspaper_indices:\n","              edges.append((article_idx, newspaper_indices[newspaper_title]))\n","              edges.append((newspaper_indices[newspaper_title], article_idx))\n","\n","    print(edges)\n","    # Create edge index tensor\n","    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n","    print(edge_index.shape)\n","    print(edge_index)\n","\n","    # Create PyG data object\n","    data = pyg.data.Data(\n","        x=node_features,  # Node features after PCA\n","        edge_index=edge_index,\n","        node_type=node_type\n","    )\n","    print(data)\n","\n","    return data, num_newspapers, newspaper_indices"],"metadata":{"id":"yIurmy4HSRfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","This code generates the homogeneous graph for the decades and topics described. It saves the graph to a specified path.\n","\"\"\"\n","\n","for decade in range(20, 70, 10):\n","  for topic in [\"labor\", \"civil-rights\", \"macro\"]:\n","    main_dataset = load_dataset(f\"amyguan/newswire-{decade}-{decade+10}\")\n","    embeddings_data = load_dataset(f\"pnsahoo/{decade}-{decade+10}-{topic}-embedding\")\n","\n","    main_dataset = main_dataset['train']\n","    embeddings_data = embeddings_data['train']\n","\n","    outlet_metadata_df = pd.read_pickle(f\"karsen_redo/DATA/outlet_metadata_{decade}{decade+10}_{topic}.pkl\")\n","\n","    newspaper_df = outlet_metadata_df.copy()\n","    newspaper_df[['latitude', 'longitude']] = newspaper_df['newspaper_coordinates'].apply(\n","      lambda x: pd.Series([float(coord) for coord in x])\n","    )\n","\n","    # Create homogeneous graph\n","    homogeneous_graph, num_newspapers, newspaper_dict = create_homogeneous_graph(embeddings_data, newspaper_df)\n","\n","    # Save the graph\n","    torch.save(homogeneous_graph, f\"karsen_redo/HOMOGNN/homogeneous_graph-{decade}-{decade+10}-{topic}-{num_newspapers}.pt\")\n","    with open(f\"karsen_redo/HOMOGNN/newspaper_node_index-{decade}-{decade+10}-{topic}.pkl\", \"wb\") as f:\n","      pickle.dump(newspaper_dict, f)\n"],"metadata":{"id":"ORGcES2nT1Mc"},"execution_count":null,"outputs":[]}]}