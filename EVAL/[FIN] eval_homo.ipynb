{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This file evaluates the homogeneous GNN models using a regression and random forest classifier. It uses the embeddings from the homogeneous GNN models to classify newspapers as Republican or Democrat, and prints and exports the results."],"metadata":{"id":"I8Jha8FMJ3ej"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDWykDd3f2cL","executionInfo":{"status":"ok","timestamp":1733968795371,"user_tz":480,"elapsed":45977,"user":{"displayName":"Karsen Lee Wahal","userId":"13626645998432039566"}},"outputId":"f19bb70a-d0ab-4903-9d36-2af6c9aa5080"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pickle\n","from google.colab import drive\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import glob\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_curve, auc\n","from matplotlib import pyplot as plt\n","\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/STANFORD/SENIOR (2024-2025)/CS224W/cs224w_project')"]},{"cell_type":"code","source":["def find_filepath(decade, topic):\n","    \"\"\"\n","    This function finds the filepath for the graph being analyzed, given a topic-decade.\n","    \"\"\"\n","    # Construct the pattern to match the file\n","    pattern = f'karsen_redo/HOMOGNN/homogeneous_graph-{decade}-{decade+10}-{topic}*.pt'\n","\n","    # Use glob to find files matching the pattern\n","    filepaths = glob.glob(pattern)\n","\n","    if filepaths:\n","        return filepaths[0]  # Return the first match\n","    else:\n","        return None\n","\n","def create_merged_df(topic, decade, path):\n","  \"\"\"\n","  This function creates a merged dataframe of embeddings and ground truths so it can be evaluated later.\n","  \"\"\"\n","  # find newspaper indices\n","  homo_embeds = torch.load(path)\n","\n","  file_path = find_filepath(decade, topic)\n","  filename = os.path.basename(file_path)\n","  num_newspapers = int(filename.split(\"-\")[-1].split(\".\")[0])\n","\n","  newspaper_indices = pd.read_pickle(f\"karsen_redo/HOMOGNN/newspaper_node_index-{decade}-{decade+10}-{topic}.pkl\")\n","\n","  # Find ground truths\n","  ground_truths = pd.read_pickle(f\"karsen_redo/DATA/clean_ground_truths.pkl\")\n","\n","  # Convert tensor to DataFrame\n","  embeds_array = homo_embeds.numpy() if isinstance(homo_embeds, torch.Tensor) else homo_embeds\n","\n","  # Create a DataFrame for embeddings\n","  embeds_df = pd.DataFrame(embeds_array)\n","  embeds_df = embeds_df.rename_axis('index').reset_index()\n","\n","  # Convert newspaper_indices to a DataFrame\n","  indices_df = pd.DataFrame(list(newspaper_indices.items()), columns=['outlet_name', 'index'])\n","  merged_indices_embeds = pd.merge(indices_df, embeds_df, on='index')\n","\n","  # Merge with ground_truths on outlet_name to get the label\n","  final_dataset = pd.merge(merged_indices_embeds, ground_truths, on='outlet_name')\n","  return final_dataset\n","\n","def regression_hetgnn(topic, decade, path, result={'hyperparameters': {}, 'model': ''}):\n","  \"\"\"\n","  This function evaluates a logistic regression model on the homogeneous GNN embeddings.\n","  \"\"\"\n","  df = create_merged_df(topic, decade, path)\n","  X = df.drop(columns=['outlet_name', 'label', 'index'])\n","  y = df['label']\n","\n","  scaler = StandardScaler()\n","  X = scaler.fit_transform(X)\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=126)\n","  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.222, random_state=126)\n","  param_grid = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # regularization strength\n","}\n","  # REGRESSION\n","  model = LogisticRegression(max_iter=1000, class_weight='balanced') # tol=1e-4, max_iter=1000\n","  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5) # , n_jobs=-1\n","  grid_search.fit(X_train, y_train)\n","\n","  best_params = grid_search.best_params_\n","  print(\"REGRESSION\\n\")\n","  print(\"------------------------\")\n","  print(\"Best hyperparameters:\", best_params)\n","\n","  best_model = grid_search.best_estimator_\n","\n","  best_model = model.fit(X_train, y_train)\n","\n","  y_val_pred = best_model.predict(X_val)\n","  val_accuracy = accuracy_score(y_val, y_val_pred)\n","  print(\"Validation Accuracy:\", val_accuracy)\n","  print(\"Validation Classification Report:\")\n","  report_val = classification_report(y_val, y_val_pred)\n","  print(report_val)\n","  roc_auc_val = roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])\n","  print(\"AUC-ROC:\", roc_auc_val)\n","\n","  fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n","  roc_auc = auc(fpr, tpr)\n","  result['fpr_val'], result['tpr_val'] = fpr, tpr\n","\n","  y_test_pred = best_model.predict(X_test)\n","  test_accuracy = accuracy_score(y_test, y_test_pred)\n","  print(\"\\nTest Accuracy:\", test_accuracy)\n","  print(\"Test Classification Report:\")\n","  report_test = classification_report(y_test, y_test_pred)\n","  print(report_test)\n","  roc_auc_test = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n","  print(\"AUC-ROC:\", roc_auc_test)\n","\n","  fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n","  roc_auc = auc(fpr, tpr)\n","  result['fpr_test'], result['tpr_test'] = fpr, tpr\n","\n","  result['reg_model'] = 'Logistic Regression'\n","  result['val_accuracy'] = val_accuracy\n","  result['test_accuracy'] = test_accuracy\n","  result['auc_val'] = roc_auc_val\n","  result['auc_test'] = roc_auc_test\n","\n","  result['report_val'] = report_val\n","  result['report_test'] = report_test\n","\n","  result['reg_hyperparameters'] = best_params\n","\n","  return result\n","\n","\n","def random_forest(topic, decade, path, result={'hyperparameters': {}, 'model': ''}):\n","    \"\"\"\n","    This function evaluates a rnadom forest classifier on the homogeneous GNN embeddings.\n","    \"\"\"\n","    df = create_merged_df(topic, decade, path)\n","    X = df.drop(columns=['outlet_name', 'label', 'index'])\n","    y = df['label']\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=126)\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.222, random_state=126)\n","\n","\n","    print(\"------------------------\")\n","    print(\"\\nRANDOM FOREST\\n\")\n","    print(\"------------------------\")\n","    param_grid = {\n","      'n_estimators': [100, 200], # 50,\n","      'max_depth': [None, 10], # 20\n","      'min_samples_split': [2, 5] # 10\n","    }\n","\n","    model = RandomForestClassifier(random_state=126, class_weight='balanced') # tol=1e-4, max_iter=1000\n","    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5) # , n_jobs=-1\n","    grid_search.fit(X_train, y_train)\n","\n","    best_params = grid_search.best_params_\n","    print(\"Best hyperparameters:\", best_params)\n","\n","    best_model = grid_search.best_estimator_\n","\n","    best_model = model.fit(X_train, y_train)\n","\n","    y_val_pred = best_model.predict(X_val)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","    print(\"Validation Accuracy:\", val_accuracy)\n","    print(\"Validation Classification Report:\")\n","    report_val = classification_report(y_val, y_val_pred)\n","    print(report_val)\n","    roc_auc_val = roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])\n","    print(\"AUC-ROC:\", roc_auc_val)\n","\n","    fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n","    roc_auc = auc(fpr, tpr)\n","    result['fpr_val'], result['tpr_val'] = fpr, tpr\n","\n","    y_test_pred = best_model.predict(X_test)\n","    test_accuracy = accuracy_score(y_test, y_test_pred)\n","    print(\"\\nTest Accuracy:\", test_accuracy)\n","    print(\"Test Classification Report:\")\n","    report_test = classification_report(y_test, y_test_pred)\n","    print(report_test)\n","    roc_auc_test = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n","    print(\"AUC-ROC:\", roc_auc_test)\n","\n","    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n","    roc_auc = auc(fpr, tpr)\n","    result['fpr_test'], result['tpr_test'] = fpr, tpr\n","\n","    result['reg_model'] = 'Random Forest'\n","    result['val_accuracy'] = val_accuracy\n","    result['test_accuracy'] = test_accuracy\n","    result['auc_val'] = roc_auc_val\n","    result['auc_test'] = roc_auc_test\n","\n","    result['report_val'] = report_val\n","    result['report_test'] = report_test\n","\n","    result['reg_hyperparameters'] = best_params\n","\n","    return result"],"metadata":{"id":"G1zJrIOUhFMq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# make table"],"metadata":{"id":"17albnbYK4Sq"}},{"cell_type":"code","source":["\"\"\"\n","Generate a markdown table from the results dictionary outputted from evaluation step.\n","\"\"\"\n","def generate_markdown_table(results):\n","    \"\"\"\n","    Converts results into a Markdown-compatible table.\n","    \"\"\"\n","    headers = [\"Model\", \"Hyperparameters\", \"Regression Model\", \"Validation Accuracy\", \"Validation AUC-ROC\", \"Test Accuracy\", \"Test AUC-ROC\"]\n","    table = f\"## {topic.title()} 19{decade}-19{decade+10} Results\\n| {' | '.join(headers)} |\\n|{'|'.join(['---'] * len(headers))}|\\n\"\n","\n","    for result in results:\n","        reg_model = result['reg_model']\n","        model = result['model']\n","        val_acc = result['val_accuracy']\n","        test_acc = result['test_accuracy']\n","        auc_val = result['auc_val']\n","        auc_test = result['auc_test']\n","        if 'hyperparameters' not in result:\n","          result[\"hyperparameters\"] = {}\n","        hyperparameters = result[\"hyperparameters\"]\n","\n","        table += f\"| {model} | ```{hyperparameters}``` | {reg_model} | {val_acc:.3f} | {auc_val:.3f} | {test_acc:.3f} | {auc_test:.3f} |\\n\"\n","\n","    return table"],"metadata":{"id":"MCbwqoc_K4_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load in results\n","with open(f'karsen_redo/EVAL/{topic}_{decade}{decade+10}/results_baseline.pkl', \"rb\") as file:\n","    results = pickle.load(file)\n","with open(f'karsen_redo/EVAL/{topic}_{decade}{decade+10}/results_homo.pkl', \"rb\") as file:\n","    results += pickle.load(file)"],"metadata":{"id":"mEMlrsGe53Kn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate and save markdown table for results\n","table = generate_markdown_table(results)\n","\n","with open(f'karsen_redo/EVAL/{topic}_{decade}{decade+10}/table_homo.txt', \"w\") as file:\n","     file.write(table)"],"metadata":{"id":"j9xXtAbuK95d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dump results into pickle\n","with open(f'karsen_redo/EVAL/{topic}_{decade}{decade+10}/results_homo.pkl', \"wb\") as file:\n","    pickle.dump(results, file)"],"metadata":{"id":"W2FKubWJTEh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# reduced table"],"metadata":{"id":"L9OGwa-HTwDc"}},{"cell_type":"code","source":["with open(f'karsen_redo/EVAL/{topic}_{decade}{decade+10}/results_homo_baseline.pkl', \"wb\") as file:\n","    pickle.dump(results, file)"],"metadata":{"id":"1GMTHrPh5sTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def final_tables(results):\n","    \"\"\"\n","    Generates separate Markdown tables for each regression model.\n","    Each table includes one row per model type, selecting the entry with the highest Validation AUC-ROC for that model type.\n","    \"\"\"\n","    # Organize results by regression model\n","    reg_model_tables = {}\n","    for result in results:\n","        reg_model = result['reg_model']\n","        if reg_model not in reg_model_tables:\n","            reg_model_tables[reg_model] = []\n","        reg_model_tables[reg_model].append(result)\n","\n","    # Prepare Markdown tables\n","    markdown = f\"## {topic.title()} 19{decade}-19{decade+10} Results\\n\\n\"\n","    headers = [\"Model\", \"Hyperparameters\", \"Validation Accuracy\", \"Validation AUC-ROC\", \"Test Accuracy\", \"Test AUC-ROC\"]\n","    header_row = f\"| {' | '.join(headers)} |\\n|{'|'.join(['---'] * len(headers))}|\\n\"\n","\n","    for reg_model, models in reg_model_tables.items():\n","        # Group models by their model type\n","        grouped_by_model = {}\n","        for model in models:\n","            model_name = model['model']\n","            if model_name not in grouped_by_model:\n","                grouped_by_model[model_name] = []\n","            grouped_by_model[model_name].append(model)\n","\n","        # Select the best result per model type\n","        best_models = [\n","            max(group, key=lambda x: x['auc_val']) for group in grouped_by_model.values()\n","        ]\n","\n","        # Create a table for this regression model\n","        table = f\"### {reg_model} Results\\n\"\n","        table += header_row\n","        for best_model in best_models:\n","            table += (\n","                f\"| {best_model['model']} | ```{best_model['hyperparameters']}``` \"\n","                f\"| {best_model['val_accuracy']:.3f} | {best_model['auc_val']:.3f} \"\n","                f\"| {best_model['test_accuracy']:.3f} | {best_model['auc_test']:.3f} |\\n\"\n","            )\n","        table += \"\\n\"\n","        markdown += table\n","\n","    return markdown"],"metadata":{"id":"D6KeMiMPTvcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Generates results for all hyperparameters and topic/decades as chosen.\n","\"\"\"\n","conv_map = {'gcn': 'Homogeneous GCN', 'gat': 'Homogeneous GATConv'}\n","\n","results = []\n","\n","sizes = [(64, 32), (128, 64)]\n","num_layers = 2\n","lr = 0.005\n","decade = 60\n","topic = 'macro'\n","for conv in ['gcn', 'gat']:\n","  for hidden_size, out_size in sizes:\n","      print(f'\\n===================\\n')\n","      print(f'MODEL {conv}, HIDDEN SIZE {hidden_size}, OUTPUT SIZE {out_size}')\n","      path = f'karsen_redo/HOMOGNN/{topic}_{decade}{decade+10}/homo_{conv}_h{hidden_size}_o{out_size}_l{num_layers}_lr{lr*1000}.pt'\n","     # path = f'karsen_redo/HETEROGNN/{topic}_{decade}{decade+10}/het_{conv}_t{temperature}_a{alpha}_h{hidden_size}_o{out_size}_l{layers}_lr{lr*1000}.pt'\n","      result = {}\n","      result['model'] = conv_map[conv]\n","      result['hyperparameters'] = {'lr': lr, 'hidden size': hidden_size, 'output size': out_size}\n","      results.append(regression_hetgnn(topic, decade, path, result))\n","\n","      result = {}\n","      result['model'] = conv_map[conv]\n","      result['hyperparameters'] = {'lr': lr, 'hidden size': hidden_size, 'output size': out_size}\n","      results.append(random_forest(topic, decade, path, result))"],"metadata":{"id":"Kh1DcINU1Obv"},"execution_count":null,"outputs":[]}]}